{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goat City: Vaccination Status Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem:\n",
    "- Goat city is ordering Covid vaccines and is fully aware that not everyone will get the vaccine. Goat city wants to know how many Covid vaccines they should be ordering. We want to show them that using a survey to predict if a person will get a vaccine is a valid way of estimating how many vaccines to get. \n",
    "\n",
    "#### Dataset: Data sourced from DataDriven Datadriven description of the dataset says:\n",
    "\n",
    ">Vaccines for H1N1 were first publicly available in the United States in October 2009, when the United States government began a vaccination campaign. We will look at data from the National 2009 H1N1 Flu Survey collected to monitor vaccination rates during that campaign. This phone survey asked people whether they had received H1N1 and seasonal flu vaccines, in conjunction with information they shared about their lives, opinions, and behaviors. A better understanding of how these characteristics have been associated with personal vaccination patterns may provide guidance for future public health efforts.\n",
    ">\n",
    "The data has already been split into a train and test set, however, we do not have access to the testing set's labels. For now, our group will be focusing ONLY on the h1n1 vaccine label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stakeholder\n",
    "    - Goat City Government/ Health Department\n",
    "-Target: H1N1\n",
    "- Cost of different errors FP/FN\n",
    "- False Positive: Model predicts they will get the vaccine, but didn't.\n",
    "Ordering too many vaccines and wasting money/material\n",
    "- False Negative: Model predicts they won't get the vaccine, but did.\n",
    "Vaccine shortage. Loss of life.\n",
    "-Metric:\n",
    "    - Recall\n",
    "    - F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in /Users/daniellerossman/Desktop/anaconda3/envs/learn-env/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /Users/daniellerossman/Desktop/anaconda3/envs/learn-env/lib/python3.8/site-packages (from catboost) (1.1.3)\n",
      "Requirement already satisfied: graphviz in /Users/daniellerossman/Desktop/anaconda3/envs/learn-env/lib/python3.8/site-packages (from catboost) (0.17)\n",
      "Requirement already satisfied: scipy in /Users/daniellerossman/Desktop/anaconda3/envs/learn-env/lib/python3.8/site-packages (from catboost) (1.5.2)\n",
      "Requirement already satisfied: six in /Users/daniellerossman/Desktop/anaconda3/envs/learn-env/lib/python3.8/site-packages (from catboost) (1.15.0)\n",
      "Requirement already satisfied: plotly in /Users/daniellerossman/Desktop/anaconda3/envs/learn-env/lib/python3.8/site-packages (from catboost) (4.11.0)\n",
      "Requirement already satisfied: matplotlib in /Users/daniellerossman/Desktop/anaconda3/envs/learn-env/lib/python3.8/site-packages (from catboost) (3.3.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /Users/daniellerossman/Desktop/anaconda3/envs/learn-env/lib/python3.8/site-packages (from catboost) (1.18.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/daniellerossman/Desktop/anaconda3/envs/learn-env/lib/python3.8/site-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/daniellerossman/Desktop/anaconda3/envs/learn-env/lib/python3.8/site-packages (from pandas>=0.24.0->catboost) (2020.1)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /Users/daniellerossman/Desktop/anaconda3/envs/learn-env/lib/python3.8/site-packages (from plotly->catboost) (1.3.3)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /Users/daniellerossman/Desktop/anaconda3/envs/learn-env/lib/python3.8/site-packages (from matplotlib->catboost) (2021.5.30)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/daniellerossman/Desktop/anaconda3/envs/learn-env/lib/python3.8/site-packages (from matplotlib->catboost) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/daniellerossman/Desktop/anaconda3/envs/learn-env/lib/python3.8/site-packages (from matplotlib->catboost) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/daniellerossman/Desktop/anaconda3/envs/learn-env/lib/python3.8/site-packages (from matplotlib->catboost) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/daniellerossman/Desktop/anaconda3/envs/learn-env/lib/python3.8/site-packages (from matplotlib->catboost) (7.2.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "import category_encoders as ce\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, roc_auc_score, plot_confusion_matrix\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "!pip install catboost\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training labels\n",
    "training_labels = pd.read_csv('../Data/training_set_labels.csv', index_col='respondent_id')\n",
    "training_features = pd.read_csv('../Data/training_set_features.csv', index_col='respondent_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c6d95fce57f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeatures_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Data/training_set_features.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlables_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Data/training_set_labels.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Training labels\n",
    "features_df = pd.read_csv('../Data/training_set_features.csv')\n",
    "lables_df = pd.read_csv('../Data/training_set_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets look into the details of each dataset. \n",
    "- The features_df and training_labels will be X within our models.\n",
    "- The lables_df and training_features will be y within our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually confirm expected results\n",
    "training_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We need to drop the 'seasonal_vaccine' column since our target will be 'h1n1_vaccine.' (This will be done below during data preparation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking features statistics\n",
    "training_features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It appears that ALL of these are categorical variables/features, because there are no \"true\" floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Data Types\n",
    "training_features.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are most likley going to have to OneHotEncode most of these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Features\n",
    "training_features.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking labels\n",
    "training_labels.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Preparation\n",
    "\n",
    "Describe and justify the process for preparing the data for analysis.\n",
    "\n",
    "Questions to consider:\n",
    "\n",
    "Were there variables you dropped or created?\n",
    "How did you address missing values or outliers?\n",
    "Why are these choices appropriate given the data and the business problem?\n",
    "Can you pipeline your preparation steps to use them consistently in the modeling process?\n",
    "\n",
    "- Our X will be all the variables in features_df. y will be the 'h1n1 vaccine' survey data from lables_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features_df\n",
    "y = lables_df[['h1n1_vaccine']] #we are dropping the \"seasonal_vaccine\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.drop(columns=['respondent_id'], inplace= True)\n",
    "'''should we leave respondent id in there since it has meaning to Goat City Health Dept.?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['health_insurance'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['health_insurance'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['health_insurance'].fillna(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['employment_industry'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['employment_occupation'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Two columns cannot be just filled in with random object. Therefore, we will work on those columns using CountEncoder and SimpleImputer, which will be covered in next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['employment_industry'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X['employment_occupation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproccessing Pipeline\n",
    "For the preproccessing, all of the columns are categorical, however, some of them are numerical, and some of them are strings. We will want to handle these these columns differently when imputing missing values.\n",
    "\n",
    "- Numerical Categories\n",
    "    - Use Sklearn's Iterative Imputer to fill in the missing values\n",
    "- String Categories\n",
    "    - Fill missing values with a new value: 'unknown'\n",
    "    - One hot encode the results\n",
    "- Categories with more then 10 unique categories\n",
    "- We will frequency code these instead, so we don't have an overwhelming amount of columns in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following process was obtained from following source, as cited:\n",
    "- Berlin L. Lindseyberlin/Cat-in-the-dat-project. GitHub. https://github.com/lindseyberlin/Cat-in-the-Dat-Project. Published October 17, 2021.\n",
    "\n",
    "- We will split the data into use set and hold set. Use set will be the one we will use to train and validate the model. Hold set will be our technical test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split use and hold \n",
    "X_use, X_hold, y_use, y_hold = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "# split train and val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_test = X_val #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize three columns\n",
    "num_cols = []\n",
    "ohe_cols = []\n",
    "freq_cols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the lists of columns\n",
    "# num = any columns with numerical value\n",
    "# ohe = any columns with object value with less than 10 unique values\n",
    "# freq = any columns with object value with 10 or more unique values\n",
    "for c in X.columns:\n",
    "    if X[c].dtype in ['float64', 'int64']:\n",
    "        num_cols.append(c)\n",
    "    elif X[c].nunique() < 10:\n",
    "        ohe_cols.append(c)\n",
    "    else:\n",
    "        freq_cols.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's check which columns are in each list. (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create pipeline for each types of cols for preprocessing.\n",
    "\n",
    "- num: scale with MinMaxScaler and apply IterativeImputer to fill the NA values.\n",
    "- ohe: apply SimpleImputer to fill NA values and encode with OneHotEncoder.\n",
    "- freq: encode with CountEncoder and apply SimpleImputer to fill the NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transformer = Pipeline(steps=[\n",
    "    ('minmaxscaler', MinMaxScaler()),\n",
    "    ('num_imputer', IterativeImputer(max_iter = 15))\n",
    "    ])\n",
    "\n",
    "ohe_transformer = Pipeline(steps=[\n",
    "    ('ohe_imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('oh_encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "freq_transformer = Pipeline(steps=[\n",
    "    ('freq_encoder', ce.count.CountEncoder(normalize=True, min_group_size=.05)),\n",
    "    ('freq_imputer', SimpleImputer(strategy='constant', fill_value=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combine 3 pipelines using ColumnTransformer to create 1 preprocessor. Then, fit the preprocessor to X_train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols),\n",
    "        ('ohe', ohe_transformer, ohe_cols),\n",
    "        ('freq', freq_transformer, freq_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Describe and justify the process for analyzing or modeling the data.\n",
    "\n",
    "Questions to consider:\n",
    "\n",
    "How will you analyze the data to arrive at an initial approach?\n",
    "How will you iterate on your initial approach to make it better?\n",
    "What model type is most appropriate, given the data and the business problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeless Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeless Baseline\n",
    "training_labels['h1n1_vaccine'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this context, a modeless baseline would have an accuracy of ~0.79 and would guess 0 every single time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual for distribution of people who received the H1N1 Vaccine\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "(training_labels['h1n1_vaccine'].value_counts().plot.bar(title=\"Respondents who Received H1N1 Vaccine\", ax=ax))\n",
    "ax.set_ylabel(\"Number of People\")\n",
    "plt.xticks([0, 1], ['Unvaccinated', 'Vaccinated'], rotation=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression\n",
    "Our first model will be logistic regression. We will start with no parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline combining preprocessor and classifier\n",
    "# classifier = LogisticRegression()\n",
    "lr_clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validate the model\n",
    "cross_validate(lr_clf, X_train, y_train, return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross validation shows that there is no overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "lr_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation set\n",
    "lr_preds = lr_clf.predict(X_val)\n",
    "X_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Based on the prediction, we will caculate the metrics: accuracy, recall, f1, and roc_auc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: {:0.3f}'.format(accuracy_score(y_val, lr_preds)))\n",
    "print('recall {:0.3f}'.format(recall_score(y_val, lr_preds)))\n",
    "print('f1: {:0.3f}'.format(f1_score(y_val, lr_preds)))\n",
    "print('roc_auc: {:0.3f}'.format(roc_auc_score(y_val , lr_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Although the accuracy is high, recall and f1 is less than 0.5. AUC might be at the higher side.\n",
    "\n",
    "- We will also generate the confusion matrix to visualize the FN/FP cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(lr_clf, X_val, y_val, cmap = \"Blues_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Naive Bayesian\n",
    "Our second model will be Gaussian Naive Bayesian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GaussianNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(nb_clf, X_train, y_train, return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross validation shows that there is no overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_preds = nb_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: {:0.3f}'.format(accuracy_score(y_val, nb_preds)))\n",
    "print('recall: {:0.3f}'.format(recall_score(y_val, nb_preds)))\n",
    "print('f1: {:0.3f}'.format(f1_score(y_val, nb_preds)))\n",
    "print('roc_auc: {:0.3f}'.format(roc_auc_score(y_val , nb_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy score is lower than Logistic Regression. However, recall, f1, ROC_AUC is higher than those of the baseline Logistic Regression Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(nb_clf, X_val, y_val, cmap=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Confusion Matrix shows that recall is higher than Precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. KNN\n",
    "\n",
    "Our third model is the K-nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', KNeighborsClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_preds = knn_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: {:0.3f}'.format(accuracy_score(y_val, knn_preds)))\n",
    "print('recall: {:0.3f}'.format(recall_score(y_val, knn_preds)))\n",
    "print('f1: {:0.3f}'.format(f1_score(y_val, knn_preds)))\n",
    "print('roc_auc: {:0.3f}'.format(roc_auc_score(y_val , knn_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Every score seems really low else than accuracy. We will not try to tune this model since the KNN takes a lot of time to run and the model score does not seem promising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Random Forest\n",
    "Our fourth model is Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(rf_clf, X_train, y_train, return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross validation score shows that the model is overfitting. The parameter tuning is needed to modify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_preds = rf_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: {:0.3f}'.format(accuracy_score(y_val, rf_preds)))\n",
    "print('recall: {:0.3f}'.format(recall_score(y_val, rf_preds)))\n",
    "print('f1: {:0.3f}'.format(f1_score(y_val, rf_preds)))\n",
    "print('roc_auc: {:0.3f}'.format(roc_auc_score(y_val , rf_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most of the score looks high enough. We might want to try out the grid search to modify the scores since the recall is the lowest out of the models we have tried (excluding KNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(rf_clf, X_val, y_val, cmap=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see the precision is really high but recall is relatively low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Decison Tree\n",
    "Our fifth model is Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_decision_tree = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(clf_decision_tree, X_train, y_train, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_decision_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_preds = clf_decision_tree.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: {:0.3f}'.format(accuracy_score(y_val, decision_tree_preds)))\n",
    "print('recall: {:0.3f}'.format(recall_score(y_val, decision_tree_preds)))\n",
    "print('f1: {:0.3f}'.format(f1_score(y_val, decision_tree_preds)))\n",
    "print('roc_auc: {:0.3f}'.format(roc_auc_score(y_val , decision_tree_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Due to the low recall score (0.477) and low f1 score (0.476), although a grid search was performed on this model, it is not included in this notebook. (It can be found [here](https://github.com/austint1121/Flatiron_GOATS_Vaccination_Prediction/blob/Danielle/Notebooks/Danielle/Project3Template-Copy1.ipynb).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Catboost\n",
    "Finally, since all of our data is categorical we will use catboost to create our sixth final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_test, _preds):\n",
    "    print('accuracy: {:0.3f}'.format(accuracy_score(y_test, _preds)))\n",
    "    print('recall: {:0.3f}'.format(recall_score(y_test, _preds)))\n",
    "    print('f1: {:0.3f}'.format(f1_score(y_test, _preds)))\n",
    "    print('roc_auc: {:0.3f}'.format(roc_auc_score(y_test , _preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('catboost_clf', CatBoostClassifier(task_type='GPU'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_clf.fit(X_train, y_train, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics(y_test, cat_clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Catboost did about as well as XGboost did, but the biggest thing I noticed is that it took 1/4 of the amount of time to train. I think this would be a much better model type to use going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Catboost Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipleline with tuned params\n",
    "tuned_cat_clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    # Changing the eval metric from \"logloss\" to \"AUC\" and modifying the learning rate\n",
    "    ('catboost_clf', CatBoostClassifier(learning_rate=0.03,\n",
    "    eval_metric='AUC', task_type='GPU'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the new model\n",
    "tuned_cat_clf.fit(X_train, y_train, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "metrics(y_test, tuned_cat_clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 More Tuned Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_cat_clf2 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    # For these parameters, I selected many common hypertuning parameters and gave them a generic value\n",
    "    ('catboost_clf2', CatBoostClassifier(eval_metric='AUC', task_type='GPU', iterations=500,\n",
    "                                         random_strength=5,\n",
    "                                         bagging_temperature=5,\n",
    "                                         max_bin=5,\n",
    "                                         grow_policy='Lossguide',\n",
    "                                         min_data_in_leaf=5,\n",
    "                                         max_depth=5,\n",
    "                                         l2_leaf_reg=10,\n",
    "                                         auto_class_weights='Balanced'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_cat_clf2.fit(X_train, y_train, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics(y_test, tuned_cat_clf2.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conclusion:\n",
    "Just from manually adding some parameters and tuning them by hand, we have a significant increase to our metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Catboost Tuning: Min Trees/Max_leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_cat_clf3 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    # For these parameters, I selected many common hypertuning parameters and gave them a generic value\n",
    "    ('catboost_clf3', CatBoostClassifier(eval_metric='AUC', task_type='GPU', iterations=500,\n",
    "                                         loss_function='Logloss',\n",
    "                                         random_strength=4,\n",
    "                                         bagging_temperature=3,\n",
    "                                         max_bin=5,\n",
    "                                         grow_policy='Lossguide',\n",
    "                                         min_data_in_leaf=5,\n",
    "                                         max_depth=5,\n",
    "                                         l2_leaf_reg=300,\n",
    "                                         auto_class_weights='Balanced',\n",
    "                                         best_model_min_trees=3,\n",
    "                                         max_leaves=30\n",
    "                                         ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_cat_clf3.fit(X_train, y_train, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics(y_test, tuned_cat_clf3.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "The evaluation of each model should accompany the creation of each model, and you should be sure to evaluate your models consistently.\n",
    "\n",
    "Evaluate how well your work solves the stated business problem.\n",
    "\n",
    "Questions to consider:\n",
    "\n",
    "How do you interpret the results?\n",
    "How well does your model fit your data? How much better is this than your baseline model? Is it over or under fit?\n",
    "How well does your model/data fit any relevant modeling assumptions?\n",
    "For the final model, you might also consider:\n",
    "\n",
    "How confident are you that your results would generalize beyond the data you have?\n",
    "How confident are you that this model would benefit the business if put into use?\n",
    "What does this final model tell you about the relationship between your inputs and outputs?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
